{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim, flatten\n",
    "from torch.nn import Linear, Conv2d, MaxPool2d, BatchNorm2d, Module, Dropout, ReLU\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "from sklearn.decomposition import PCA \n",
    "from numpy.linalg import eig\n",
    "#from autograd_lib import autograd_lib\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0.01,1,300)\n",
    "x=x[:,None]\n",
    "y = np.sin(5 * 3.142 * x)/(5 * 3.142 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(1,30)\n",
    "        self.fc2 = nn.Linear(30,70)\n",
    "        self.fc3 = nn.Linear(70,1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Gradient Norm\n",
    "def grad_norm():\n",
    "    grad_all = 0.0\n",
    "    for p in model.parameters():\n",
    "        grad =0.0\n",
    "        if p.grad is not None:\n",
    "            grad = (p.grad.cpu().data.numpy() ** 2).sum()\n",
    "        grad_all+=grad\n",
    "    grad_norm = grad_all**0.5\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnloss(loss):\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =1000\n",
    "minimal_ratio = []\n",
    "model = Network()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "errorloss =[]\n",
    "costVector = []\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = loss_fn(output, y)\n",
    "    loss.backward()\n",
    "    #loss_norm = gradloss(torch.tensor(0.0))\n",
    "    loss = loss_fn(torch.tensor(grad_norm()),torch.tensor(0.0))\n",
    "    optimizer.step()\n",
    "\n",
    "    h = torch.autograd.functional.hessian(returnloss, loss)\n",
    "    h_eig = torch.symeig(h).eigenvalues \n",
    "    \n",
    "    tot_eig = 0\n",
    "    A = 0\n",
    "    for i in h_eig:\n",
    "        if i > 0:\n",
    "            A = i\n",
    "            tot_eig +=i \n",
    "    minimal_ratio.append(A/tot_eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
